{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:37:39.186181Z","iopub.execute_input":"2022-12-16T12:37:39.187002Z","iopub.status.idle":"2022-12-16T12:37:39.210551Z","shell.execute_reply.started":"2022-12-16T12:37:39.186882Z","shell.execute_reply":"2022-12-16T12:37:39.209583Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def sigmoid(z):\n    return 1 / (1 + np.exp(-z))\ndef relu(z):\n    return np.maximum(0, z)\ndef tanh(z):\n    return np.tanh(z)\ndef softmax(z):\n    return (np.exp(z - np.max(z)) / np.exp(x - np.max(x)).sum())","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:37:39.212422Z","iopub.execute_input":"2022-12-16T12:37:39.213197Z","iopub.status.idle":"2022-12-16T12:37:39.219609Z","shell.execute_reply.started":"2022-12-16T12:37:39.213153Z","shell.execute_reply":"2022-12-16T12:37:39.218630Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def initialize_parameters(layer_dims):\n    \"\"\"\n    Inputs:\n    layer_dims - list of number of neurons in each layer\n    Returns:\n    Wl - Weight matrix of shape (layer_dims[l], layer_dims[l-1])\n    b1 - Bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    parameters = {}\n    L = len(layer_dims) # total no. of layers\n    \n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n    \n    return parameters","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:37:39.226140Z","iopub.execute_input":"2022-12-16T12:37:39.226449Z","iopub.status.idle":"2022-12-16T12:37:39.233415Z","shell.execute_reply.started":"2022-12-16T12:37:39.226420Z","shell.execute_reply":"2022-12-16T12:37:39.232139Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def forward_propagation(A_prev, W, b, activation):\n    \"\"\"\n    Inputs:\n    A_prev - previous layer activation\n    W - weight, b - bias\n    Returns:\n    A - post-activation\n    \"\"\"\n    if activation == \"sigmoid\":\n        Z = np.dot(W, A_prev) + b\n        A = sigmoid(Z)\n        linear_cache = (A_prev, W, b)\n        activation_cache = Z\n    \n    elif activation == \"tanh\":\n        Z = np.dot(W, A_prev) + b\n        A = tanh(Z)\n        linear_cache = (A_prev, W, b)\n        activation_cache = Z\n        \n    elif activation == \"relu\":\n        Z = np.dot(W, A_prev) + b\n        A = relu(Z)\n        linear_cache = (A_prev, W, b)\n        activation_cache = Z\n    \n    elif activation == \"softmax\":\n        Z = np.dot(W, A_prev) + b\n        A = softmax(Z)\n        linear_cache = (A_prev, W, b)\n        activation_cache = Z\n        \n    cache = (linear_cache, activation_cache)\n    \n    return A, cache","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:56:20.273717Z","iopub.execute_input":"2022-12-16T12:56:20.274149Z","iopub.status.idle":"2022-12-16T12:56:20.284090Z","shell.execute_reply.started":"2022-12-16T12:56:20.274111Z","shell.execute_reply":"2022-12-16T12:56:20.282805Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def L_forward_propagation(X, parameters, activation_hidden, activation_output):\n    \"\"\"\n    Inputs:\n    X - input layer\n    parameters - weights and biases of L layers\n    activation_hidden - hidden layer activation function\n    activation_output - output layer activation function\n    Returns:\n    AL - activation of the output layer\n    caches - linear and activation cache to back propagate easily\n    \"\"\"\n    caches = []\n    A = X\n    L = len(parameters) // 2\n    \n    for l in range(1, L):\n        A_prev = A\n        if activation_hidden == 'relu':\n            A, cache = forward_propagation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n            caches.append(cache)\n        elif activation_hidden == 'tanh':\n            A, cache = forward_propagation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'tanh')\n            caches.append(cache)\n            \n    if activation_output == 'softmax':\n        AL, cache = forward_propagation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'softmax')\n        caches.append(cache)\n    elif activation_output == 'sigmoid':\n        AL, cache = forward_propagation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'sigmoid')\n        caches.append(cache)\n    \n    return AL, caches","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:56:33.513012Z","iopub.execute_input":"2022-12-16T12:56:33.513425Z","iopub.status.idle":"2022-12-16T12:56:33.523258Z","shell.execute_reply.started":"2022-12-16T12:56:33.513390Z","shell.execute_reply":"2022-12-16T12:56:33.522088Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def compute_cost(AL, Y):\n    \"\"\"\n    Inputs:\n    AL - output layer activation\n    Y - true labels\n    Returns:\n    cost - cross-entropy cost\n    \"\"\"\n    \n    m = Y.shape[1]\n    cost = - (1 / m) * (np.dot(Y, np.log(AL).T) + np.dot(1-Y, np.log(1-AL).T))\n    cost = np.squeeze(cost)\n    \n    return cost","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:37:39.264016Z","iopub.execute_input":"2022-12-16T12:37:39.264427Z","iopub.status.idle":"2022-12-16T12:37:39.274217Z","shell.execute_reply.started":"2022-12-16T12:37:39.264394Z","shell.execute_reply":"2022-12-16T12:37:39.273136Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def linear_backward_propagation(dZ, cache):\n    \"\"\"\n    Inputs:\n    dZ - Gradient of the cost with respect to Z\n    cache - (A_prev, W, b) from the forward propapagtion\n    Returns:\n    dA_prev - Gradient of the cost w.r.t. A_prev\n    dW - Gradient of the cost w.r.t. W\n    db - Gradient of the cost w.r.t. b \n    \"\"\"\n    \n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    \n    dw = 1 / m * np.dot(dZ, A_prev.T)\n    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n    \n    return dA_prev, dW, db","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:37:39.276539Z","iopub.execute_input":"2022-12-16T12:37:39.277033Z","iopub.status.idle":"2022-12-16T12:37:39.284481Z","shell.execute_reply.started":"2022-12-16T12:37:39.276991Z","shell.execute_reply":"2022-12-16T12:37:39.283404Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def activation_backward_propagation(dA, cache, activation):\n    \"\"\"\n    Inputs:\n    dA - post activation gradient\n    cache - (linear_cache, activation_cache)\n    activation - activation function to be used\n    Returns:\n    dA_prev - Gradient of the cost w.r.t. A_prev\n    dW - Gradient of the cost w.r.t. W\n    db - Gradient of the cost w.r.t. b \n    \"\"\"\n    \n    linear_cache, activation_cache = cache\n    \n    if activation == 'sigmoid':\n        s = sigmoid(activation_cache)\n        dZ = dA * s * (1-s)\n        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n    \n    elif activation == 'relu':\n        dZ = np.array(dA, copy=True)\n        dZ[activation_cache <= 0] = 0\n        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n    \n    elif activation == 'tanh':\n        Z = tanh(activation_cache)\n        dZ = 1 - Z*Z\n        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n    \n    elif activation == 'softmax':\n        tmp = activation_cache.reshape((-1,1))\n        dZ = np.diagflat(activation_cache) - np.dot(tmp, tmp.T)\n        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n    \n    return dA_prev, dW, db","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:57:03.515918Z","iopub.execute_input":"2022-12-16T12:57:03.516328Z","iopub.status.idle":"2022-12-16T12:57:03.526006Z","shell.execute_reply.started":"2022-12-16T12:57:03.516293Z","shell.execute_reply":"2022-12-16T12:57:03.524789Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def L_backward_propagation(AL, Y, caches, activation):\n    \"\"\"\n    Inputs:\n    AL - ouput of L_model_forward()\n    Y - true label vector\n    caches - (linear_cache, activation_cache)\n    Returns:\n    grads - gradients of dAl, dWl, dbl, where l is the layer number\n    \"\"\"\n    \n    grads = {}\n    L = len(caches)\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) # Y is same shape as AL\n    \n    dAL = ","metadata":{},"execution_count":null,"outputs":[]}]}