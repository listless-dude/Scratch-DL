{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/daskoushik/scratch-mlp?scriptVersionId=114003138\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","execution_count":1,"id":"4e3c8659","metadata":{"execution":{"iopub.execute_input":"2022-12-16T15:37:17.452077Z","iopub.status.busy":"2022-12-16T15:37:17.45131Z","iopub.status.idle":"2022-12-16T15:37:17.462762Z","shell.execute_reply":"2022-12-16T15:37:17.461657Z"},"papermill":{"duration":0.020889,"end_time":"2022-12-16T15:37:17.465466","exception":false,"start_time":"2022-12-16T15:37:17.444577","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"id":"5da23b75","metadata":{"execution":{"iopub.execute_input":"2022-12-16T15:37:17.474836Z","iopub.status.busy":"2022-12-16T15:37:17.474411Z","iopub.status.idle":"2022-12-16T15:37:17.481535Z","shell.execute_reply":"2022-12-16T15:37:17.480262Z"},"papermill":{"duration":0.014445,"end_time":"2022-12-16T15:37:17.483871","exception":false,"start_time":"2022-12-16T15:37:17.469426","status":"completed"},"tags":[]},"outputs":[],"source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","def relu(z):\n","    return np.maximum(0, z)\n","def tanh(z):\n","    return np.tanh(z)\n","def softmax(z):\n","    return (np.exp(z - np.max(z)) / np.exp(x - np.max(x)).sum())"]},{"cell_type":"code","execution_count":3,"id":"b6e1bd04","metadata":{"execution":{"iopub.execute_input":"2022-12-16T15:37:17.493493Z","iopub.status.busy":"2022-12-16T15:37:17.493063Z","iopub.status.idle":"2022-12-16T15:37:17.499977Z","shell.execute_reply":"2022-12-16T15:37:17.498837Z"},"papermill":{"duration":0.014719,"end_time":"2022-12-16T15:37:17.502486","exception":false,"start_time":"2022-12-16T15:37:17.487767","status":"completed"},"tags":[]},"outputs":[],"source":["def initialize_parameters(layer_dims):\n","    \"\"\"\n","    Inputs:\n","    layer_dims - list of number of neurons in each layer\n","    Returns:\n","    Wl - Weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","    b1 - Bias vector of shape (layer_dims[l], 1)\n","    \"\"\"\n","    parameters = {}\n","    L = len(layer_dims) # total no. of layers\n","    \n","    for l in range(1, L):\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","    \n","    return parameters"]},{"cell_type":"code","execution_count":4,"id":"b1b17b4b","metadata":{"execution":{"iopub.execute_input":"2022-12-16T15:37:17.515319Z","iopub.status.busy":"2022-12-16T15:37:17.514871Z","iopub.status.idle":"2022-12-16T15:37:17.533612Z","shell.execute_reply":"2022-12-16T15:37:17.532303Z"},"papermill":{"duration":0.029184,"end_time":"2022-12-16T15:37:17.536543","exception":false,"start_time":"2022-12-16T15:37:17.507359","status":"completed"},"tags":[]},"outputs":[],"source":["def forward_propagation(A_prev, W, b, activation):\n","    \"\"\"\n","    Inputs:\n","    A_prev - previous layer activation\n","    W - weight, b - bias\n","    Returns:\n","    A - post-activation\n","    \"\"\"\n","    if activation == \"sigmoid\":\n","        Z = np.dot(W, A_prev) + b\n","        A = sigmoid(Z)\n","        linear_cache = (A_prev, W, b)\n","        activation_cache = Z\n","    \n","    elif activation == \"tanh\":\n","        Z = np.dot(W, A_prev) + b\n","        A = tanh(Z)\n","        linear_cache = (A_prev, W, b)\n","        activation_cache = Z\n","        \n","    elif activation == \"relu\":\n","        Z = np.dot(W, A_prev) + b\n","        A = relu(Z)\n","        linear_cache = (A_prev, W, b)\n","        activation_cache = Z\n","    \n","    elif activation == \"softmax\":\n","        Z = np.dot(W, A_prev) + b\n","        A = softmax(Z)\n","        linear_cache = (A_prev, W, b)\n","        activation_cache = Z\n","        \n","    cache = (linear_cache, activation_cache)\n","    \n","    return A, cache"]},{"cell_type":"code","execution_count":5,"id":"6cf68a04","metadata":{"execution":{"iopub.execute_input":"2022-12-16T15:37:17.546571Z","iopub.status.busy":"2022-12-16T15:37:17.545789Z","iopub.status.idle":"2022-12-16T15:37:17.555001Z","shell.execute_reply":"2022-12-16T15:37:17.554138Z"},"papermill":{"duration":0.016984,"end_time":"2022-12-16T15:37:17.557619","exception":false,"start_time":"2022-12-16T15:37:17.540635","status":"completed"},"tags":[]},"outputs":[],"source":["def L_forward_propagation(X, parameters, activation_hidden, activation_output):\n","    \"\"\"\n","    Inputs:\n","    X - input layer\n","    parameters - weights and biases of L layers\n","    activation_hidden - hidden layer activation function\n","    activation_output - output layer activation function\n","    Returns:\n","    AL - activation of the output layer\n","    caches - linear and activation cache to back propagate easily\n","    \"\"\"\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2\n","    \n","    for l in range(1, L):\n","        A_prev = A\n","        if activation_hidden == 'relu':\n","            A, cache = forward_propagation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n","            caches.append(cache)\n","        elif activation_hidden == 'tanh':\n","            A, cache = forward_propagation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'tanh')\n","            caches.append(cache)\n","            \n","    if activation_output == 'softmax':\n","        AL, cache = forward_propagation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'softmax')\n","        caches.append(cache)\n","    elif activation_output == 'sigmoid':\n","        AL, cache = forward_propagation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'sigmoid')\n","        caches.append(cache)\n","    \n","    return AL, caches"]},{"cell_type":"code","execution_count":6,"id":"ea8a1463","metadata":{"execution":{"iopub.execute_input":"2022-12-16T15:37:17.566937Z","iopub.status.busy":"2022-12-16T15:37:17.566487Z","iopub.status.idle":"2022-12-16T15:37:17.572441Z","shell.execute_reply":"2022-12-16T15:37:17.571285Z"},"papermill":{"duration":0.013299,"end_time":"2022-12-16T15:37:17.574813","exception":false,"start_time":"2022-12-16T15:37:17.561514","status":"completed"},"tags":[]},"outputs":[],"source":["def compute_cost(AL, Y):\n","    \"\"\"\n","    Inputs:\n","    AL - output layer activation\n","    Y - true labels\n","    Returns:\n","    cost - cross-entropy cost\n","    \"\"\"\n","    \n","    m = Y.shape[1]\n","    cost = - (1 / m) * (np.dot(Y, np.log(AL).T) + np.dot(1-Y, np.log(1-AL).T))\n","    cost = np.squeeze(cost)\n","    \n","    return cost"]},{"cell_type":"code","execution_count":7,"id":"63e9233d","metadata":{"execution":{"iopub.execute_input":"2022-12-16T15:37:17.584413Z","iopub.status.busy":"2022-12-16T15:37:17.583923Z","iopub.status.idle":"2022-12-16T15:37:17.590288Z","shell.execute_reply":"2022-12-16T15:37:17.58917Z"},"papermill":{"duration":0.013879,"end_time":"2022-12-16T15:37:17.5927","exception":false,"start_time":"2022-12-16T15:37:17.578821","status":"completed"},"tags":[]},"outputs":[],"source":["def linear_backward_propagation(dZ, cache):\n","    \"\"\"\n","    Inputs:\n","    dZ - Gradient of the cost with respect to Z\n","    cache - (A_prev, W, b) from the forward propapagtion\n","    Returns:\n","    dA_prev - Gradient of the cost w.r.t. A_prev\n","    dW - Gradient of the cost w.r.t. W\n","    db - Gradient of the cost w.r.t. b \n","    \"\"\"\n","    \n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","    \n","    dw = 1 / m * np.dot(dZ, A_prev.T)\n","    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n","    dA_prev = np.dot(W.T, dZ)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":8,"id":"efc52f0a","metadata":{"execution":{"iopub.execute_input":"2022-12-16T15:37:17.601974Z","iopub.status.busy":"2022-12-16T15:37:17.601564Z","iopub.status.idle":"2022-12-16T15:37:17.610561Z","shell.execute_reply":"2022-12-16T15:37:17.609601Z"},"papermill":{"duration":0.016227,"end_time":"2022-12-16T15:37:17.612759","exception":false,"start_time":"2022-12-16T15:37:17.596532","status":"completed"},"tags":[]},"outputs":[],"source":["def activation_backward_propagation(dA, cache, activation):\n","    \"\"\"\n","    Inputs:\n","    dA - post activation gradient\n","    cache - (linear_cache, activation_cache)\n","    activation - activation function to be used\n","    Returns:\n","    dA_prev - Gradient of the cost w.r.t. A_prev\n","    dW - Gradient of the cost w.r.t. W\n","    db - Gradient of the cost w.r.t. b \n","    \"\"\"\n","    \n","    linear_cache, activation_cache = cache\n","    \n","    if activation == 'sigmoid':\n","        s = sigmoid(activation_cache)\n","        dZ = dA * s * (1-s)\n","        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n","    \n","    elif activation == 'relu':\n","        dZ = np.array(dA, copy=True)\n","        dZ[activation_cache <= 0] = 0\n","        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n","    \n","    elif activation == 'tanh':\n","        Z = tanh(activation_cache)\n","        dZ = 1 - Z*Z\n","        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n","    \n","    elif activation == 'softmax':\n","        tmp = activation_cache.reshape((-1,1))\n","        dZ = np.diagflat(activation_cache) - np.dot(tmp, tmp.T)\n","        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":9,"id":"6741f8d1","metadata":{"execution":{"iopub.execute_input":"2022-12-16T15:37:17.621965Z","iopub.status.busy":"2022-12-16T15:37:17.62155Z","iopub.status.idle":"2022-12-16T15:37:17.632531Z","shell.execute_reply":"2022-12-16T15:37:17.631637Z"},"papermill":{"duration":0.018392,"end_time":"2022-12-16T15:37:17.63502","exception":false,"start_time":"2022-12-16T15:37:17.616628","status":"completed"},"tags":[]},"outputs":[],"source":["def L_backward_propagation(AL, Y, caches, activation_hidden, activation_output):\n","    \"\"\"\n","    Inputs:\n","    AL - ouput of L_model_forward()\n","    Y - true label vector\n","    caches - (linear_cache, activation_cache)\n","    Returns:\n","    grads - gradients of dAl, dWl, dbl, where l is the layer number\n","    \"\"\"\n","    \n","    grads = {}\n","    L = len(caches)\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # Y is same shape as AL\n","    \n","    if activation_output == 'sigmoid':\n","        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","        current_cache = caches[L-1]\n","        dA_prev_temp, dW_temp, db_temp = activation_backward_propagation(dAL, current_cache, \"sigmoid\")\n","        grads[\"dA\" + str(L-1)] = dA_prev_temp\n","        grads[\"dW\" + str(L)] = dW_temp\n","        grads[\"db\" + str(L)] = db_temp\n","    elif activation_output == 'softmax':\n","        dAL = AL - Y\n","        current_cache = caches[L-1]\n","        dA_prev_temp, dW_temp, db_temp = activation_backward_propagation(dAL, current_cache, \"softmax\")\n","        grads[\"dA\" + str(L-1)] = dA_prev_temp\n","        grads[\"dW\" + str(L)] = dW_temp\n","        grads[\"db\" + str(L)] = db_temp\n","        \n","    for l in reversed(range(L-1)):\n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = activation_backward_propagation(dA_prev_temp, current_cache, activation_hidden)\n","        grads[\"dA\" + str(l)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","        \n","    return grads"]},{"cell_type":"code","execution_count":10,"id":"60f16c41","metadata":{"execution":{"iopub.execute_input":"2022-12-16T15:37:17.644896Z","iopub.status.busy":"2022-12-16T15:37:17.644116Z","iopub.status.idle":"2022-12-16T15:37:17.651496Z","shell.execute_reply":"2022-12-16T15:37:17.650401Z"},"papermill":{"duration":0.015059,"end_time":"2022-12-16T15:37:17.654086","exception":false,"start_time":"2022-12-16T15:37:17.639027","status":"completed"},"tags":[]},"outputs":[],"source":["def update_parameters(params, grads, learning_rate):\n","    \"\"\"\n","    Inputs:\n","    params - parameters dictionary\n","    grads - gradients dictionary\n","    Returns:\n","    parameters - updated parameters dictionary\n","    \"\"\"\n","    parameters = params.copy()\n","    L = len(parameters) // 2\n","    \n","    for l in range(L):\n","        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n","        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n","    \n","    return parameters"]},{"cell_type":"code","execution_count":11,"id":"51b89aab","metadata":{"execution":{"iopub.execute_input":"2022-12-16T15:37:17.663918Z","iopub.status.busy":"2022-12-16T15:37:17.662841Z","iopub.status.idle":"2022-12-16T15:37:17.672192Z","shell.execute_reply":"2022-12-16T15:37:17.671392Z"},"papermill":{"duration":0.0167,"end_time":"2022-12-16T15:37:17.674602","exception":false,"start_time":"2022-12-16T15:37:17.657902","status":"completed"},"tags":[]},"outputs":[],"source":["def L_layer_model(X, Y, layers_dims, activation_hidden, activation_output, learning_rate = 0.001, num_iterations=2000, print_cost=False):\n","    \"\"\"\n","    Inputs:\n","    X - input data matrix\n","    Y - true labels vector\n","    layers_dims - list containing the input size and each layer size\n","    learning_rate - for gradient descent update\n","    num_iterations - epoch of gradient descent\n","    print_cost - prints cost of each step\n","    Returns:\n","    parameters - learnt parameters by the model\n","    \"\"\"\n","    costs = []\n","    parameters = initialize_parameters(layers_dims)\n","    \n","    for i in range(0, num_iterations):\n","        # Forward propagation\n","        AL, caches = L_forward_propagation(X, parameters, activation_hidden, activation_output)\n","        \n","        cost = compute_cost(AL, Y)\n","        \n","        grads = L_backward_propagation(AL, Y, caches, activation_hidden, activation_output)\n","        \n","        parameters = update_parameters(parameters, grads, learning_rate)\n","        \n","        if print_cost and i % 100 -- 0 or i == num_iterations - 1:\n","            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n","        if i % 100 == 0 or i == num_iterations:\n","            costs.append(cost)\n","    \n","    return parameters, costs"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":9.925381,"end_time":"2022-12-16T15:37:18.299218","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-12-16T15:37:08.373837","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}