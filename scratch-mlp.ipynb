{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/daskoushik/scratch-mlp?scriptVersionId=114021832\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"3c635d00","metadata":{"papermill":{"duration":0.005755,"end_time":"2022-12-16T21:14:42.874812","exception":false,"start_time":"2022-12-16T21:14:42.869057","status":"completed"},"tags":[]},"source":["# Packages"]},{"cell_type":"code","execution_count":1,"id":"5c6428e5","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:42.88714Z","iopub.status.busy":"2022-12-16T21:14:42.886378Z","iopub.status.idle":"2022-12-16T21:14:42.897507Z","shell.execute_reply":"2022-12-16T21:14:42.896524Z"},"papermill":{"duration":0.019909,"end_time":"2022-12-16T21:14:42.90005","exception":false,"start_time":"2022-12-16T21:14:42.880141","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","id":"1f30fb63","metadata":{"papermill":{"duration":0.004223,"end_time":"2022-12-16T21:14:42.908965","exception":false,"start_time":"2022-12-16T21:14:42.904742","status":"completed"},"tags":[]},"source":["# Activation functions"]},{"cell_type":"code","execution_count":2,"id":"b1b35202","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:42.920231Z","iopub.status.busy":"2022-12-16T21:14:42.919293Z","iopub.status.idle":"2022-12-16T21:14:42.926651Z","shell.execute_reply":"2022-12-16T21:14:42.925733Z"},"papermill":{"duration":0.015817,"end_time":"2022-12-16T21:14:42.929253","exception":false,"start_time":"2022-12-16T21:14:42.913436","status":"completed"},"tags":[]},"outputs":[],"source":["def sigmoid(Z):\n","    cache = Z\n","    A = 1 / (1 + np.exp(-Z))\n","    return A, cache\n","def relu(Z):\n","    cache = Z\n","    A = np.maximum(0, Z)\n","    return A, cache\n","def tanh(Z):\n","    cache = Z\n","    A = np.tanh(Z)\n","    return A, cache\n","def softmax(Z):\n","    cache = Z\n","    A = np.exp(Z - np.max(Z)) / np.exp(Z - np.max(Z)).sum()\n","    return A, cache"]},{"cell_type":"markdown","id":"407baeb3","metadata":{"papermill":{"duration":0.004534,"end_time":"2022-12-16T21:14:42.93855","exception":false,"start_time":"2022-12-16T21:14:42.934016","status":"completed"},"tags":[]},"source":["# Model helper functions:\n","\n","* **initialize_parameters** - initializes at random the parameters weights and biases\n","* **forward_propagation** - for linear->activation forward prop\n","* **L_forward_propagation** - for L layers - hidden: linear->activation_hidden, linear->activation_output\n","* **compute_cost** - cross-entropy loss\n","* **linear_backward_propagation** - each layer linear back prop\n","* **activation_backward_propagation** - each layer activation back prop\n","* **L_backward_propagation** - for L layers back prop\n","* **update_parameters** - update the parameters"]},{"cell_type":"code","execution_count":3,"id":"05ed1202","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:42.949606Z","iopub.status.busy":"2022-12-16T21:14:42.949122Z","iopub.status.idle":"2022-12-16T21:14:42.956486Z","shell.execute_reply":"2022-12-16T21:14:42.955232Z"},"papermill":{"duration":0.015789,"end_time":"2022-12-16T21:14:42.958936","exception":false,"start_time":"2022-12-16T21:14:42.943147","status":"completed"},"tags":[]},"outputs":[],"source":["def initialize_parameters(layer_dims):\n","    \"\"\"\n","    Inputs:\n","    layer_dims - list of number of neurons in each layer\n","    Returns:\n","    Wl - Weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","    b1 - Bias vector of shape (layer_dims[l], 1)\n","    \"\"\"\n","    parameters = {}\n","    L = len(layer_dims) # total no. of layers\n","    \n","    for l in range(1, L):\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","    \n","    return parameters"]},{"cell_type":"code","execution_count":4,"id":"609a385c","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:42.970114Z","iopub.status.busy":"2022-12-16T21:14:42.969508Z","iopub.status.idle":"2022-12-16T21:14:42.975586Z","shell.execute_reply":"2022-12-16T21:14:42.974597Z"},"papermill":{"duration":0.014445,"end_time":"2022-12-16T21:14:42.978015","exception":false,"start_time":"2022-12-16T21:14:42.96357","status":"completed"},"tags":[]},"outputs":[],"source":["def linear_forward_propagation(A, W, b):\n","    \"\"\"\n","    Inputs: \n","    A - activation from previous layer\n","    W - weight, b - bias\n","    Returns:\n","    Z - input of activation or pre-activation parameter\n","    cache - tuple (A, W, b)\n","    \"\"\"\n","    \n","    Z = np.dot(A, W) + b\n","    cache = (A, W, b)\n","    \n","    return Z, cache"]},{"cell_type":"code","execution_count":5,"id":"75d67a44","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:42.989545Z","iopub.status.busy":"2022-12-16T21:14:42.989097Z","iopub.status.idle":"2022-12-16T21:14:43.003395Z","shell.execute_reply":"2022-12-16T21:14:43.002262Z"},"papermill":{"duration":0.02325,"end_time":"2022-12-16T21:14:43.006018","exception":false,"start_time":"2022-12-16T21:14:42.982768","status":"completed"},"tags":[]},"outputs":[],"source":["def activation_forward_propagation(A_prev, W, b, activation):\n","    \"\"\"\n","    Inputs:\n","    A_prev - previous layer activation\n","    W - weight, b - bias\n","    Returns:\n","    A - post-activation\n","    \"\"\"\n","    if activation == \"sigmoid\":\n","        Z, linear_cache = linear_forward_propagation(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z)\n","    \n","    elif activation == \"tanh\":\n","        Z, linear_cache = linear_forward_propagation(A_prev, W, b)\n","        A, activation_cache = tanh(Z)\n","        \n","    elif activation == \"relu\":\n","        Z, linear_cache = linear_forward_propagation(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","    \n","    elif activation == \"softmax\":\n","        Z, linear_cache = linear_forward_propagation(A_prev, W, b)\n","        A, activation_cache = softmax(Z)\n","        \n","    cache = (linear_cache, activation_cache)\n","    \n","    return A, cache"]},{"cell_type":"code","execution_count":6,"id":"9530789e","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:43.01675Z","iopub.status.busy":"2022-12-16T21:14:43.01633Z","iopub.status.idle":"2022-12-16T21:14:43.026707Z","shell.execute_reply":"2022-12-16T21:14:43.025473Z"},"papermill":{"duration":0.018544,"end_time":"2022-12-16T21:14:43.029127","exception":false,"start_time":"2022-12-16T21:14:43.010583","status":"completed"},"tags":[]},"outputs":[],"source":["def L_forward_propagation(X, parameters, activation_hidden, activation_output):\n","    \"\"\"\n","    Inputs:\n","    X - input layer\n","    parameters - weights and biases of L layers\n","    activation_hidden - hidden layer activation function\n","    activation_output - output layer activation function\n","    Returns:\n","    AL - activation of the output layer\n","    caches - linear and activation cache to back propagate easily\n","    \"\"\"\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2\n","    \n","    for l in range(1, L):\n","        A_prev = A\n","        if activation_hidden == 'relu':\n","            A, cache = activation_forward_propagation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n","            caches.append(cache)\n","        elif activation_hidden == 'tanh':\n","            A, cache = activation_forward_propagation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'tanh')\n","            caches.append(cache)\n","            \n","    if activation_output == 'softmax':\n","        AL, cache = activation_forward_propagation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'softmax')\n","        caches.append(cache)\n","    elif activation_output == 'sigmoid':\n","        AL, cache = activation_forward_propagation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'sigmoid')\n","        caches.append(cache)\n","    \n","    return AL, caches"]},{"cell_type":"code","execution_count":7,"id":"f50f3e6c","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:43.040127Z","iopub.status.busy":"2022-12-16T21:14:43.039685Z","iopub.status.idle":"2022-12-16T21:14:43.046535Z","shell.execute_reply":"2022-12-16T21:14:43.045393Z"},"papermill":{"duration":0.015302,"end_time":"2022-12-16T21:14:43.048935","exception":false,"start_time":"2022-12-16T21:14:43.033633","status":"completed"},"tags":[]},"outputs":[],"source":["def compute_cost(AL, Y):\n","    \"\"\"\n","    Inputs:\n","    AL - output layer activation\n","    Y - true labels\n","    Returns:\n","    cost - cross-entropy cost\n","    \"\"\"\n","    \n","    m = Y.shape[1]\n","    cost = - (1./m) * (np.dot(Y, np.log(AL).T) + np.dot(1-Y, np.log(1-AL).T))\n","    cost = np.squeeze(cost)\n","    \n","    return cost"]},{"cell_type":"code","execution_count":8,"id":"5510945d","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:43.059789Z","iopub.status.busy":"2022-12-16T21:14:43.059365Z","iopub.status.idle":"2022-12-16T21:14:43.066639Z","shell.execute_reply":"2022-12-16T21:14:43.065462Z"},"papermill":{"duration":0.015445,"end_time":"2022-12-16T21:14:43.068956","exception":false,"start_time":"2022-12-16T21:14:43.053511","status":"completed"},"tags":[]},"outputs":[],"source":["def linear_backward_propagation(dZ, cache):\n","    \"\"\"\n","    Inputs:\n","    dZ - Gradient of the cost with respect to Z\n","    cache - (A_prev, W, b) from the forward propapagtion\n","    Returns:\n","    dA_prev - Gradient of the cost w.r.t. A_prev\n","    dW - Gradient of the cost w.r.t. W\n","    db - Gradient of the cost w.r.t. b \n","    \"\"\"\n","    \n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","    \n","    dW = 1./ m * np.dot(dZ, A_prev.T)\n","    db = 1./ m * np.sum(dZ, axis=1, keepdims=True)\n","    dA_prev = np.dot(W.T, dZ)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":9,"id":"3cd78ce3","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:43.079796Z","iopub.status.busy":"2022-12-16T21:14:43.079355Z","iopub.status.idle":"2022-12-16T21:14:43.089025Z","shell.execute_reply":"2022-12-16T21:14:43.087819Z"},"papermill":{"duration":0.018045,"end_time":"2022-12-16T21:14:43.091488","exception":false,"start_time":"2022-12-16T21:14:43.073443","status":"completed"},"tags":[]},"outputs":[],"source":["def activation_backward_propagation(dA, cache, activation):\n","    \"\"\"\n","    Inputs:\n","    dA - post activation gradient\n","    cache - (linear_cache, activation_cache)\n","    activation - activation function to be used\n","    Returns:\n","    dA_prev - Gradient of the cost w.r.t. A_prev\n","    dW - Gradient of the cost w.r.t. W\n","    db - Gradient of the cost w.r.t. b \n","    \"\"\"\n","    \n","    linear_cache, activation_cache = cache\n","    \n","    if activation == 'sigmoid':\n","        s = 1 / (1 + np.exp(-activation_cache))\n","        dZ = dA * s * (1-s)\n","        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n","    \n","    elif activation == 'relu':\n","        dZ = np.array(dA, copy=True)\n","        dZ[activation_cache <= 0] = 0\n","        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n","    \n","    elif activation == 'tanh':\n","        Z = tanh(activation_cache)\n","        dZ = 1 - Z*Z\n","        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n","    \n","    elif activation == 'softmax':\n","        tmp = activation_cache.reshape((-1,1))\n","        dZ = np.diagflat(activation_cache) - np.dot(tmp, tmp.T)\n","        dA_prev, dW, db = linear_backward_propagation(dZ, linear_cache)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":10,"id":"1ab2e045","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:43.102781Z","iopub.status.busy":"2022-12-16T21:14:43.102398Z","iopub.status.idle":"2022-12-16T21:14:43.11384Z","shell.execute_reply":"2022-12-16T21:14:43.112903Z"},"papermill":{"duration":0.019739,"end_time":"2022-12-16T21:14:43.115976","exception":false,"start_time":"2022-12-16T21:14:43.096237","status":"completed"},"tags":[]},"outputs":[],"source":["def L_backward_propagation(AL, Y, caches, activation_hidden, activation_output):\n","    \"\"\"\n","    Inputs:\n","    AL - ouput of L_model_forward()\n","    Y - true label vector\n","    caches - (linear_cache, activation_cache)\n","    Returns:\n","    grads - gradients of dAl, dWl, dbl, where l is the layer number\n","    \"\"\"\n","    \n","    grads = {}\n","    L = len(caches)\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # Y is same shape as AL\n","    \n","    if activation_output == 'sigmoid':\n","        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","        current_cache = caches[L-1]\n","        dA_prev_temp, dW_temp, db_temp = activation_backward_propagation(dAL, current_cache, 'sigmoid')\n","        grads[\"dA\" + str(L-1)] = dA_prev_temp\n","        grads[\"dW\" + str(L)] = dW_temp\n","        grads[\"db\" + str(L)] = db_temp\n","    elif activation_output == 'softmax':\n","        dAL = AL - Y\n","        current_cache = caches[L-1]\n","        dA_prev_temp, dW_temp, db_temp = activation_backward_propagation(dAL, current_cache, 'softmax')\n","        grads[\"dA\" + str(L-1)] = dA_prev_temp\n","        grads[\"dW\" + str(L)] = dW_temp\n","        grads[\"db\" + str(L)] = db_temp\n","        \n","    for l in reversed(range(L-1)):\n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = activation_backward_propagation(grads[\"dA\" + str(l+1)], current_cache, activation_hidden)\n","        grads[\"dA\" + str(l)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","        \n","    return grads"]},{"cell_type":"code","execution_count":11,"id":"30bdda6a","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:43.127841Z","iopub.status.busy":"2022-12-16T21:14:43.127447Z","iopub.status.idle":"2022-12-16T21:14:43.134382Z","shell.execute_reply":"2022-12-16T21:14:43.133023Z"},"papermill":{"duration":0.016265,"end_time":"2022-12-16T21:14:43.136896","exception":false,"start_time":"2022-12-16T21:14:43.120631","status":"completed"},"tags":[]},"outputs":[],"source":["def update_parameters(params, grads, learning_rate):\n","    \"\"\"\n","    Inputs:\n","    params - parameters dictionary\n","    grads - gradients dictionary\n","    Returns:\n","    parameters - updated parameters dictionary\n","    \"\"\"\n","    parameters = params.copy()\n","    L = len(parameters) // 2\n","    \n","    for l in range(L):\n","        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n","        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n","    \n","    return parameters"]},{"cell_type":"markdown","id":"b32f2a66","metadata":{"papermill":{"duration":0.004283,"end_time":"2022-12-16T21:14:43.146139","exception":false,"start_time":"2022-12-16T21:14:43.141856","status":"completed"},"tags":[]},"source":["# L-layer Model:\n","Using all the helper functions to create the model"]},{"cell_type":"code","execution_count":12,"id":"cfc1739f","metadata":{"execution":{"iopub.execute_input":"2022-12-16T21:14:43.157095Z","iopub.status.busy":"2022-12-16T21:14:43.156665Z","iopub.status.idle":"2022-12-16T21:14:43.165144Z","shell.execute_reply":"2022-12-16T21:14:43.164032Z"},"papermill":{"duration":0.01696,"end_time":"2022-12-16T21:14:43.167613","exception":false,"start_time":"2022-12-16T21:14:43.150653","status":"completed"},"tags":[]},"outputs":[],"source":["def L_layer_model(X, Y, layers_dims, activation_hidden, activation_output, learning_rate = 0.001, num_iterations=2000, print_cost=False):\n","    \"\"\"\n","    Inputs:\n","    X - input data matrix\n","    Y - true labels vector\n","    layers_dims - list containing the input size and each layer size\n","    learning_rate - for gradient descent update\n","    num_iterations - epoch of gradient descent\n","    print_cost - prints cost of each step\n","    Returns:\n","    parameters - learnt parameters by the model\n","    \"\"\"\n","    costs = []\n","    parameters = initialize_parameters(layers_dims)\n","    \n","    for i in range(0, num_iterations):\n","        # Forward propagation\n","        AL, caches = L_forward_propagation(X, parameters, activation_hidden, activation_output)\n","        \n","        cost = compute_cost(AL, Y)\n","        \n","        grads = L_backward_propagation(AL, Y, caches, activation_hidden, activation_output)\n","        \n","        parameters = update_parameters(parameters, grads, learning_rate)\n","        \n","        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n","            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n","        if i % 100 == 0 or i == num_iterations:\n","            costs.append(cost)\n","    \n","    return parameters, costs"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":10.410074,"end_time":"2022-12-16T21:14:43.89413","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-12-16T21:14:33.484056","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}